{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for numerical experiments with hPAM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import of necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomotopy as to\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data from a csv file, which contains a lemmatized dataset (for example, Amazon dataset)\n",
    "df = pd.read_csv('Amazon_lem.csv', header=None, names=['docs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of unique words 31486\n"
     ]
    }
   ],
   "source": [
    "#extract unique words and calculate their number\n",
    "uniq_words = list(filter(lambda x: x, set(df.docs.str.cat(sep=' ').strip().split(' '))))\n",
    "uniq_words_len = len(uniq_words)\n",
    "print('the number of unique words is', uniq_words_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform documents into word lists\n",
    "words_in_docs = list(map(lambda x: x.split(), df.docs.dropna().values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting hPAM model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for the loop\n",
    "\n",
    "# Set the number of super topics (second hierarchical level)\n",
    "level1_topics = [2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,33,34,36,38,40,42,44,46,48,50,52, 54, 56, 58,60]\n",
    "\n",
    "# Set the number of subtopics (third hierarchical level)\n",
    "level2_topics = [2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,33,34,36,38,40,42,44,46,48,50,52, 54, 56, 58,60]\n",
    "\n",
    "# set values of parameter eta \n",
    "eta_level = [0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1]\n",
    " \n",
    "#set the initial value of parameter alpha\n",
    "alpha = 0.0001\n",
    "\n",
    "#set the name of the file for recording the results\n",
    "myfile = 'HPAM_newRenyi_WoS_lem_run1.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training hPAM models in a loop for selected parameters and calculating Renyi entropy, log-likelihood, and perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myfile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1d45166828c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     file.write('eta' +';' + 'alpha' +';' + 'Num topics (lev 1)' + ';' +  'WRD (lev 1)' + ';' + 'prob_sum (lev 1)' + ';' + \n\u001b[0;32m     13\u001b[0m                \u001b[1;34m'Renyi(lev 1)'\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m';'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'Num topics (lev 2)'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m';'\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'WRD (lev 2)'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m';'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'prob_sum (lev 2)'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m';'\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'myfile' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate Renyi entropy for each level of hPAM model\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "def process_words_probs(x):\n",
    "    words = list(map(lambda y: y[0], x))\n",
    "    probs = list(map(lambda y: y[1], x))\n",
    "    return words, probs\n",
    "\n",
    "with open(myfile, 'a') as file: \n",
    "    file.write('eta' +';' + 'alpha' +';' + 'Num topics (lev 1)' + ';' +  'WRD (lev 1)' + ';' + 'prob_sum (lev 1)' + ';' + \n",
    "               'Renyi(lev 1)' +';' + 'Num topics (lev 2)' + ';' +'WRD (lev 2)' + ';' + 'prob_sum (lev 2)' + ';' + \n",
    "             'Renyi(lev 2)' +';'+ 'perplexity' + ';' + 'Log-likelihood'  + '\\n')\n",
    "\n",
    "for eta in eta_level:\n",
    "    Renyi_lev1 = []\n",
    "    topic_lev1 = []\n",
    "    \n",
    "    #organize cycle for the second hierarchical level\n",
    "    for ilevel1 in level1_topics:\n",
    "        #cycle for the third hierarchical level   \n",
    "        for ilevel2 in level2_topics:\n",
    "            # Model initialization\n",
    "            hpam = to.HPAModel(k1=ilevel1, k2=ilevel2, alpha=alpha, eta=eta)\n",
    "            list(map(hpam.add_doc, words_in_docs))\n",
    "       \n",
    "            # train the model\n",
    "            # set the number of iterations\n",
    "            hpam.train(iter=100, workers = 10)\n",
    "        \n",
    "            #obtain a list of lists (word - probability) for each topic from 0 to level1_topics + level2_topics\n",
    "            words_topics_distr = list(map(lambda x: hpam.get_topic_words(x, uniq_words_len), range(1 + ilevel1 + ilevel2)))\n",
    "            df_words_probs = pd.DataFrame(np.zeros((uniq_words_len, 2 + ilevel1 + ilevel2)))\n",
    "            df_words_probs.columns = ['words'] + df_words_probs.columns.to_list()[:-1]\n",
    "            df_words_probs['words'] = uniq_words\n",
    "            df_words_probs.set_index('words', inplace=True)\n",
    "        \n",
    "            for i, words_topics_col in enumerate(words_topics_distr):\n",
    "                words, probs = process_words_probs(words_topics_col)\n",
    "                df_words_probs.loc[words, i] = probs\n",
    "        \n",
    "            num_word, num_col = df_words_probs.shape\n",
    "  #------------------------------------------------------------------------------------------------------      \n",
    "            # calculation of Renyi entropy for level 2 \n",
    "            #set the threshold\n",
    "            thresh=1/num_word\n",
    "            word_ratio1 = 0\n",
    "            sum_prob1 = 0                \n",
    "       \n",
    "            myprob1 = df_words_probs.values[:, 1:ilevel1+1]\n",
    "            num_word, num_col = myprob1.shape\n",
    "            \n",
    "            #select the maximum probability for each word \n",
    "            mymax = myprob1.max(axis=1)\n",
    "            # mymax contains the list of maximum probabilities \n",
    "            # select the probabilities larger than thresh\n",
    "            mywrd = mymax[mymax>thresh]\n",
    "            # calculate the number of such values\n",
    "            word_ratio1 = len(mywrd)\n",
    "            # calculate the sum of those probabilities\n",
    "            sum_prob1 = sum(mywrd)\n",
    "            #Shannon entropy\n",
    "            Sh1  = np.log(word_ratio1/(num_word*ilevel1))\n",
    "            #Internal energy\n",
    "            En1 = -np.log(sum_prob1/ilevel1)\n",
    "            #Free energy\n",
    "            Fen1 = En1 - Sh1*ilevel1\n",
    "            #Renyi entropy\n",
    "            if ilevel1==1: \n",
    "                Renyi1 = Fen1/(ilevel1)\n",
    "            else: Renyi1 = Fen1/(ilevel1-1)\n",
    "   #--------------------------------------------------------------------------------------------------------                         \n",
    "            # calculation of Renyi entropy for level 3 \n",
    "            \n",
    "            myprob2 = df_words_probs.values[:, ilevel1+1:]\n",
    "            word_ratio2 = 0\n",
    "            sum_prob2 = 0\n",
    "            \n",
    "            # select the maximum probability for each word\n",
    "            mymax2 = myprob2.max(axis=1)\n",
    "            # mymax contains the list of maximum probabilities \n",
    "            #select the probabilities larger than thresh\n",
    "            mywrd2 = mymax2[mymax2>thresh]\n",
    "            #calculate the number of such values\n",
    "            word_ratio2 = len(mywrd2)\n",
    "            #calculate the sum of those probabilities\n",
    "            sum_prob2 = sum(mywrd2)\n",
    "            #Shannon entropy\n",
    "            Sh2  = np.log(word_ratio2/(num_word*ilevel2))\n",
    "            #Internal energy\n",
    "            En2 = -np.log(sum_prob2/ilevel2)\n",
    "            #Free energy\n",
    "            Fen2 = En2 - Sh2*ilevel2\n",
    "            #Renyi entropy\n",
    "            if ilevel2==1: \n",
    "                Renyi2 = Fen2/(ilevel2)\n",
    "            else: Renyi2 = Fen2/(ilevel2-1)\n",
    "            \n",
    "            print('eta: ', eta, ' ','topics level2: ', ilevel2, ' ', word_ratio2, ' ', sum_prob2, ' ', Renyi2)\n",
    "        \n",
    "    #---------------------------------------------------------------------------------------------------------    \n",
    "        # recording the results to a csv file\n",
    "            #calculate \\rho and \\tilde{P} for level 2\n",
    "            wrd1_lev1 = word_ratio1/(num_word*ilevel1)\n",
    "            sum_prob1_lev1 = (sum_prob1/ilevel1)\n",
    "            \n",
    "            #calculate \\rho and \\tilde{P} for level 3\n",
    "            wrd1_lev2 = word_ratio2/(num_word*ilevel2)\n",
    "            sum_prob2_lev2 = (sum_prob2/ilevel2)\n",
    "        \n",
    "            with open(myfile, 'a') as file: \n",
    "                file.write(str(eta) + ';' + str(alpha) + ';' + str(ilevel1) + ';' \n",
    "                           + str(wrd1_lev1) + ';' + str(sum_prob1_lev1) + ';'    \n",
    "                                 + str(Renyi1) +';' + str(ilevel2) + ';' \n",
    "                                 + str(wrd1_lev2) + ';' + str(sum_prob2_lev2) + ';'\n",
    "                             + str(Renyi2) + ';'  + str(hpam.perplexity) +';' + str(hpam.ll_per_word) + '\\n')\n",
    "                                                                                                                       \n",
    "        \n",
    "        \n",
    "stop = time.time()\n",
    "print('time of execution (sec)', stop - start)\n",
    "print('---------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
